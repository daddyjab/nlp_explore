{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow: Basic Text Classification - Binary Label\n",
    "* Reference: https://www.tensorflow.org/tutorials/keras/text_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io, os, re, shutil\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the IMDB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "84131840/84125825 [==============================] - 10s 0us/step\n",
      "84140032/84125825 [==============================] - 10s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Download the dataset to be used\n",
    "IMDB_MOVIE_REVIEW_DATA_URL = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "dataset = tf.keras.utils.get_file(\n",
    "    \"aclImdb_v1.tar.gz\", IMDB_MOVIE_REVIEW_DATA_URL,\n",
    "    untar=True, cache_dir='.', cache_subdir='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the directory name of the dataset\n",
    "dataset_dir = os.path.join( os.path.dirname(dataset), 'aclImdb' )\n",
    "# os.listdir(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the directory name of the training data\n",
    "train_dir = os.path.join(dataset_dir, 'train')\n",
    "# os.listdir(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove directory containing unsupervised examples\n",
    "remove_dir = os.path.join(train_dir, 'unsup')\n",
    "shutil.rmtree(remove_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 20000 files for training.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "Using 5000 files for validation.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "seed = 42\n",
    "raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "            'aclImdb/train', batch_size=batch_size,\n",
    "            validation_split=0.2, subset='training',\n",
    "            seed=seed)\n",
    "\n",
    "raw_val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "            'aclImdb/train', batch_size=batch_size,\n",
    "            validation_split=0.2, subset='validation',\n",
    "            seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review b'\"Pandemonium\" is a horror movie spoof that comes off more stupid than funny. Believe me when I tell you, I love comedies. Especially comedy spoofs. \"Airplane\", \"The Naked Gun\" trilogy, \"Blazing Saddles\", \"High Anxiety\", and \"Spaceballs\" are some of my favorite comedies that spoof a particular genre. \"Pandemonium\" is not up there with those films. Most of the scenes in this movie had me sitting there in stunned silence because the movie wasn\\'t all that funny. There are a few laughs in the film, but when you watch a comedy, you expect to laugh a lot more than a few times and that\\'s all this film has going for it. Geez, \"Scream\" had more laughs than this film and that was more of a horror film. How bizarre is that?<br /><br />*1/2 (out of four)'\n",
      "Label 0\n",
      "Review b\"David Mamet is a very interesting and a very un-equal director. His first movie 'House of Games' was the one I liked best, and it set a series of films with characters whose perspective of life changes as they get into complicated situations, and so does the perspective of the viewer.<br /><br />So is 'Homicide' which from the title tries to set the mind of the viewer to the usual crime drama. The principal characters are two cops, one Jewish and one Irish who deal with a racially charged area. The murder of an old Jewish shop owner who proves to be an ancient veteran of the Israeli Independence war triggers the Jewish identity in the mind and heart of the Jewish detective.<br /><br />This is were the flaws of the film are the more obvious. The process of awakening is theatrical and hard to believe, the group of Jewish militants is operatic, and the way the detective eventually walks to the final violent confrontation is pathetic. The end of the film itself is Mamet-like smart, but disappoints from a human emotional perspective.<br /><br />Joe Mantegna and William Macy give strong performances, but the flaws of the story are too evident to be easily compensated.\"\n",
      "Label 0\n"
     ]
    }
   ],
   "source": [
    "for text_batch, label_batch in raw_train_ds.take(1):\n",
    "  for i in range(2):\n",
    "    print(\"Review\", text_batch.numpy()[i])\n",
    "    print(\"Label\", label_batch.numpy()[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0 corresponds to neg\n",
      "Label 1 corresponds to pos\n"
     ]
    }
   ],
   "source": [
    "print(\"Label 0 corresponds to\", raw_train_ds.class_names[0])\n",
    "print(\"Label 1 corresponds to\", raw_train_ds.class_names[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "raw_test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "            'aclImdb/test', batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom standardization function to strip HTML break tags '<br />'.\n",
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "    stripped_punct = tf.strings.regex_replace(stripped_html, '[%s]' % re.escape(string.punctuation), '')\n",
    "    return stripped_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary size (features) and number of words in a sequence (review).\n",
    "max_features = 10000\n",
    "sequence_length = 250\n",
    "\n",
    "# Use the text vectorization layer to normalize, split, and map strings to\n",
    "# integers. Note that the layer uses the custom standardization defined above.\n",
    "# Set maximum_sequence length as all samples are not of the same length.\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "                    standardize=custom_standardization,\n",
    "                    max_tokens=max_features,\n",
    "                    output_mode='int',\n",
    "                    output_sequence_length=sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
    "train_text = raw_train_ds.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(text, label):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return vectorize_layer(text), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review tf.Tensor(b'I love horror movies that brings out a real amount of mystery like say \"silent hill\" ( which i found to be quite good, but still, was missing something ) and movies that keeps you guessing, this i thought was one of those movies. At first the movie starts out with some really good suspense and builds up a good starting point for a good horror scene, but after that it just rolls down the hill and from there it only goes faster and faster down. I mentioned silent hill at first for a reason because i can see a lot of \"stolen\" themes from that movie in here.. All in all i would say, watch silent hill instead of this one, its better, its more scary, it has a lot more suspense and also the ending is a lot better.. And best of all, you wont feel ripped off as i did with this one.. This just seems to be one of those \"i like that movie so I\\'m gonna re-make it in my own really bad version\" kinda movie.. Oh and one more thing... Lordi.. in a horror movie... thats like trying to scare a kid with a care bear who has \"hug me and i will love you forever\" written on the stomach of it..', shape=(), dtype=string)\n",
      "Label neg\n",
      "Vectorized review (<tf.Tensor: shape=(1, 250), dtype=int64, numpy=\n",
      "array([[  10,  115,  192,   91,   12,  905,   44,    4,  145, 1137,    5,\n",
      "         744,   38,  131, 1286, 2164,   60,   10,  251,    6,   26,  176,\n",
      "          49,   18,  125,   13,  960,  138,    3,   91,   12,  913,   22,\n",
      "        2945,   11,   10,  197,   13,   28,    5,  143,   91,   31,   83,\n",
      "           2,   17,  501,   44,   16,   46,   62,   49,  844,    3, 3985,\n",
      "          56,    4,   49, 1822,  212,   15,    4,   49,  192,  134,   18,\n",
      "         101,   12,    9,   40, 7745,  185,    2, 2164,    3,   35,   47,\n",
      "           9,   61,  261, 4663,    3, 4663,  185,   10, 1057, 1286, 2164,\n",
      "          31,   83,   15,    4,  273,   84,   10,   68,   67,    4,  171,\n",
      "           5, 2688, 1300,   35,   12,   17,    8,  128,   30,    8,   30,\n",
      "          10,   59,  131,  103, 1286, 2164,  291,    5,   11,   28,   29,\n",
      "         122,   29,   50,  620,    9,   43,    4,  171,   50,  844,    3,\n",
      "          77,    2,  270,    7,    4,  171,  122,    3,  113,    5,   30,\n",
      "          22,  464,  232, 3249,  127,   14,   10,  117,   16,   11,   28,\n",
      "          11,   40,  180,    6,   26,   28,    5,  143,   10,   38,   12,\n",
      "          17,   37,  141, 2079,  957,    9,    8,   54,  196,   62,   80,\n",
      "         310, 1809,   17,  449,    3,   28,   50,  150, 9296,    8,    4,\n",
      "         192,   17,  177,   38,  258,    6, 2259,    4,  555,   16,    4,\n",
      "         438, 2170,   36,   43, 8867,   69,    3,   10,   76,  115,   22,\n",
      "        1447,  418,   20,    2, 2921,    5,    9,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0]], dtype=int64)>, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n"
     ]
    }
   ],
   "source": [
    "# retrieve a batch (of 32 reviews and labels) from the dataset\n",
    "text_batch, label_batch = next(iter(raw_train_ds))\n",
    "first_review, first_label = text_batch[0], label_batch[0]\n",
    "print(\"Review\", first_review)\n",
    "print(\"Label\", raw_train_ds.class_names[first_label])\n",
    "print(\"Vectorized review\", vectorize_text(first_review, first_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1287 --->  lovely\n",
      " 313 --->  american\n",
      "Vocabulary size: 10000\n"
     ]
    }
   ],
   "source": [
    "print(\"1287 ---> \",vectorize_layer.get_vocabulary()[1287])\n",
    "print(\" 313 ---> \",vectorize_layer.get_vocabulary()[313])\n",
    "print('Vocabulary size: {}'.format(len(vectorize_layer.get_vocabulary())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vectorized training, validation, and test datasets\n",
    "train_ds = raw_train_ds.map(vectorize_text)\n",
    "val_ds = raw_val_ds.map(vectorize_text)\n",
    "test_ds = raw_test_ds.map(vectorize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([[   1,   35,  215, ...,    0,    0,    0],\n",
       "         [3089,  226,    3, ...,    0,    0,    0],\n",
       "         [  10,  139,   25, ...,    0,    0,    0],\n",
       "         ...,\n",
       "         [  85,  686,   42, ...,    0,    0,    0],\n",
       "         [   2, 3136,   65, ...,    0,    0,    0],\n",
       "         [   4,  181,  246, ...,    0,    0,    0]], dtype=int64),\n",
       "  array([0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,\n",
       "         0, 0, 0, 0, 0, 0, 1, 1, 0, 0]))]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ x for x in train_ds.take(1).as_numpy_iterator() ]\n",
    "# print(dir(train_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the Dataset for Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 16)          160016    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, None, 16)          0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 160,033\n",
      "Trainable params: 160,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 16\n",
    "model = tf.keras.Sequential( [\n",
    "    layers.Embedding( max_features+1, embedding_dim ),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.GlobalAveragePooling1D(),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile and Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared log dir: .\\logs\\fit_20211118-160535\n"
     ]
    }
   ],
   "source": [
    "# tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir='logs')\n",
    "LOGS_ROOTDIR = os.path.join('.', 'logs')\n",
    "this_run_log_dir = os.path.join( LOGS_ROOTDIR, \"fit_\" + dt.datetime.now().strftime(\"%Y%m%d-%H%M%S\") )\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=this_run_log_dir, histogram_freq=1)\n",
    "print(f\"Prepared log dir: {this_run_log_dir}\")\n",
    "\n",
    "# Stop epochs if validation loss is increasing\n",
    "earlystopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=losses.BinaryCrossentropy(from_logits=True),\n",
    "    metrics=tf.metrics.BinaryAccuracy(threshold=0.0)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 6s 8ms/step - loss: 0.1063 - binary_accuracy: 0.9667 - val_loss: 0.3370 - val_binary_accuracy: 0.8776\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.1030 - binary_accuracy: 0.9674 - val_loss: 0.3424 - val_binary_accuracy: 0.8776\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.0980 - binary_accuracy: 0.9708 - val_loss: 0.3489 - val_binary_accuracy: 0.8784\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.0934 - binary_accuracy: 0.9731 - val_loss: 0.3555 - val_binary_accuracy: 0.8778\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.0892 - binary_accuracy: 0.9742 - val_loss: 0.3609 - val_binary_accuracy: 0.8786\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.0861 - binary_accuracy: 0.9757 - val_loss: 0.3688 - val_binary_accuracy: 0.8770\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.0824 - binary_accuracy: 0.9768 - val_loss: 0.3754 - val_binary_accuracy: 0.8774\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.0794 - binary_accuracy: 0.9776 - val_loss: 0.3843 - val_binary_accuracy: 0.8750\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.0756 - binary_accuracy: 0.9798 - val_loss: 0.3929 - val_binary_accuracy: 0.8740\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.0712 - binary_accuracy: 0.9810 - val_loss: 0.4008 - val_binary_accuracy: 0.8740\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=epochs,\n",
    "    callbacks=[tensorboard_callback, earlystopping_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 1s 954us/step - loss: 0.4803 - binary_accuracy: 0.8500\n",
      "Loss:  0.480336993932724\n",
      "Accuracy:  0.8500400185585022\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_ds)\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'binary_accuracy', 'val_loss', 'val_binary_accuracy'])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.lines.Line2D at 0x16eb4d24fd0>"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot( 'binary_accuracy', 'b-', data=history.history )\n",
    "plt.plot( 'val_binary_accuracy', 'g:', data=history.history )\n",
    "# plt.plot( [accuracy]*len(history.history['binary_accuracy']), 'r--', data=history.history )\n",
    "y_axis_min_val = max( 0.5, round(min(accuracy, min(history.history['val_binary_accuracy']), min(history.history['binary_accuracy']) )-0.1, 1) )\n",
    "_ = plt.ylim( y_axis_min_val, 1.0)\n",
    "plt.axhline( accuracy, color='r', linestyle=':')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 23932), started 1 day, 1:11:45 ago. (Use '!kill 23932' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-e0511be812d6afe9\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-e0511be812d6afe9\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "# %reload_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text_vectorization_3 (TextVe (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "sequential (Sequential)      (None, 1)                 160033    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 160,033\n",
      "Trainable params: 160,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Wrap the trained model to permit it to be used directly in input strings (vs. vectors)\n",
    "# and then apply an activation layer to yield a floating result between 0 and 1\n",
    "export_model = tf.keras.Sequential([\n",
    "    vectorize_layer,\n",
    "    model,\n",
    "    layers.Activation('sigmoid')\n",
    "    ])\n",
    "\n",
    "export_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=losses.BinaryCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 10s 12ms/step - loss: 0.4803 - accuracy: 0.8500\n",
      "0.8500400185585022\n"
     ]
    }
   ],
   "source": [
    "# Test it with `raw_test_ds`, which yields raw strings\n",
    "loss, accuracy = export_model.evaluate(raw_test_ds)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This movies is 8 of 10 stars.</td>\n",
       "      <td>0.826285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This movies is 7 of 10 stars.</td>\n",
       "      <td>0.778393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This movies is 10 of 10 stars.</td>\n",
       "      <td>0.663508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This movies is 5 of 10 stars.</td>\n",
       "      <td>0.626547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The movie was great!</td>\n",
       "      <td>0.617263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>This movies is 9 of 10 stars.</td>\n",
       "      <td>0.614599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>This movies is 2 of 10 stars.</td>\n",
       "      <td>0.569792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This movies is 5 stars.</td>\n",
       "      <td>0.554164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>This movies is all good.</td>\n",
       "      <td>0.546045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>This movies is 2 stars.</td>\n",
       "      <td>0.495270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>This movies is not at all good.</td>\n",
       "      <td>0.488269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>This movies is 1 of 10 stars.</td>\n",
       "      <td>0.452340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>This movies is 3 of 10 stars.</td>\n",
       "      <td>0.431825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>This movies is 1 stars.</td>\n",
       "      <td>0.379625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>This movies is 4 of 10 stars.</td>\n",
       "      <td>0.367805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>This movies is 3 stars.</td>\n",
       "      <td>0.360238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>This movies is 4 stars.</td>\n",
       "      <td>0.301205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>This movies is 6 of 10 stars.</td>\n",
       "      <td>0.281934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>The movie was okay.</td>\n",
       "      <td>0.268144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>The movie was terrible...</td>\n",
       "      <td>0.186375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Review    Rating\n",
       "0     This movies is 8 of 10 stars.  0.826285\n",
       "1     This movies is 7 of 10 stars.  0.778393\n",
       "2    This movies is 10 of 10 stars.  0.663508\n",
       "3     This movies is 5 of 10 stars.  0.626547\n",
       "4              The movie was great!  0.617263\n",
       "5     This movies is 9 of 10 stars.  0.614599\n",
       "6     This movies is 2 of 10 stars.  0.569792\n",
       "7           This movies is 5 stars.  0.554164\n",
       "8          This movies is all good.  0.546045\n",
       "9           This movies is 2 stars.  0.495270\n",
       "10  This movies is not at all good.  0.488269\n",
       "11    This movies is 1 of 10 stars.  0.452340\n",
       "12    This movies is 3 of 10 stars.  0.431825\n",
       "13          This movies is 1 stars.  0.379625\n",
       "14    This movies is 4 of 10 stars.  0.367805\n",
       "15          This movies is 3 stars.  0.360238\n",
       "16          This movies is 4 stars.  0.301205\n",
       "17    This movies is 6 of 10 stars.  0.281934\n",
       "18              The movie was okay.  0.268144\n",
       "19        The movie was terrible...  0.186375"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples = [\n",
    "  \"This movies is 10 of 10 stars.\",\n",
    "  \"This movies is 9 of 10 stars.\",\n",
    "  \"This movies is 8 of 10 stars.\",\n",
    "  \"This movies is 7 of 10 stars.\",\n",
    "  \"This movies is 6 of 10 stars.\",\n",
    "  \"This movies is 5 of 10 stars.\",\n",
    "  \"This movies is 4 of 10 stars.\",\n",
    "  \"This movies is 3 of 10 stars.\",\n",
    "  \"This movies is 2 of 10 stars.\",\n",
    "  \"This movies is 1 of 10 stars.\",\n",
    "  \"This movies is all good.\",\n",
    "  \"This movies is not at all good.\",\n",
    "  \"This movies is 5 stars.\",\n",
    "  \"This movies is 4 stars.\",\n",
    "  \"This movies is 3 stars.\",\n",
    "  \"This movies is 2 stars.\",\n",
    "  \"This movies is 1 stars.\",\n",
    "  \"The movie was great!\",\n",
    "  \"The movie was okay.\",\n",
    "  \"The movie was terrible...\"\n",
    "]\n",
    "\n",
    "preds = export_model.predict(examples).flatten()\n",
    "r_df = pd.DataFrame( {'Review': examples, 'Rating': preds} )\n",
    "r_df = r_df.sort_values(by='Rating', ascending=False).reset_index(drop=True)\n",
    "r_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve the Trained Word Embeddings and Save them to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.get_layer('embedding').get_weights()[0]\n",
    "vocab = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with io.open('./data/vectors.tsv', 'w', encoding='utf-8') as out_v:\n",
    "    with io.open('./data/metadata.tsv', 'w', encoding='utf-8') as out_m:\n",
    "        for index, word in enumerate(vocab):\n",
    "            if index == 0:\n",
    "                continue    # skip 0, it's padding\n",
    "                \n",
    "            vec = weights[index]\n",
    "            out_v.write( '\\t'.join( [ str(x) for x in vec ]) + '\\n' )\n",
    "            out_m.write( word + '\\n' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Embeddings\n",
    "* Tensorflow Embedding Projector: http://projector.tensorflow.org/\n",
    "* Click on \"Load data\".\n",
    "* Upload the two files you created above: vecs.tsv and meta.tsv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a32e172698e50824da0c33bd5d955f0bc05a5b3df854aad51c6c7dfa51d03fde"
  },
  "kernelspec": {
   "display_name": "Python [conda env:py38]",
   "language": "python",
   "name": "conda-env-py38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

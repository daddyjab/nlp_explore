{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow: Basic Text Classification - Multiclass Label, with Hyperparameter Tuning via HParams\n",
    "\n",
    "Reference:\n",
    "* https://www.tensorflow.org/tutorials/keras/text_classification#exercise_multi-class_classification_on_stack_overflow_questions\n",
    "* https://www.tensorflow.org/tensorboard/hyperparameter_tuning_with_hparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io, os, re, shutil\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.display.float_format = '{:,.3f}'.format\n",
    "pd.options.display.max_rows = 100\n",
    "pd.options.display.max_columns = 300\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, losses\n",
    "\n",
    "# Hyperparameter tuner using Keras Tuner\n",
    "import keras_tuner as kt\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Download the Stack Overflow Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare to download the dataset to be used to the location CACHE_DIR/DATASET_SUBDIR\n",
    "CACHE_DIR = '.\\\\data'\n",
    "DATASET_URL = \"http://storage.googleapis.com/download.tensorflow.org/data/stack_overflow_16k.tar.gz\"\n",
    "DATASET_SUBDIR = 'stackoverflowdb'\n",
    "DATASET_ZIPFILE = 'stack_overflow_16k.tar.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the targeted dataset\n",
    "# dataset = tf.keras.utils.get_file(\n",
    "#             DATASET_ZIPFILE, DATASET_URL,\n",
    "#             untar=True, cache_dir=CACHE_DIR, cache_subdir=DATASET_SUBDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\data\\stackoverflowdb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['README.md', 'stack_overflow_16k.tar.gz', 'test', 'train']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dir = os.path.join( CACHE_DIR, DATASET_SUBDIR )\n",
    "print(dataset_dir)\n",
    "os.listdir(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\data\\stackoverflowdb\\train\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['csharp', 'java', 'javascript', 'python']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the directory name of the training data\n",
    "train_dir = os.path.join(dataset_dir, 'train')\n",
    "print(train_dir)\n",
    "os.listdir(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\data\\stackoverflowdb\\test\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['csharp', 'java', 'javascript', 'python']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dir = os.path.join(dataset_dir, 'test')\n",
    "print(test_dir)\n",
    "os.listdir(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove directory containing unsupervised examples\n",
    "# remove_dir = os.path.join(train_dir, 'unsup')\n",
    "# shutil.rmtree(remove_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 files belonging to 4 classes.\n",
      "Using 6400 files for training.\n",
      "Found 8000 files belonging to 4 classes.\n",
      "Using 1600 files for validation.\n",
      "Found 8000 files belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "seed = 42\n",
    "raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "            train_dir, batch_size=batch_size,\n",
    "            validation_split=0.2, subset='training',\n",
    "            seed=seed)\n",
    "\n",
    "raw_val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "            train_dir, batch_size=batch_size,\n",
    "            validation_split=0.2, subset='validation',\n",
    "            seed=seed)\n",
    "\n",
    "raw_test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "            test_dir, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 0 => Class: csharp\n",
      "Label: 1 => Class: java\n",
      "Label: 2 => Class: javascript\n",
      "Label: 3 => Class: python\n"
     ]
    }
   ],
   "source": [
    "for i,c in enumerate(raw_train_ds.class_names):\n",
    "    print(f\"Label: {i} => Class: {c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for text_batch, label_batch in raw_train_ds.take(1):\n",
    "#   for i in range(1):\n",
    "#     print(f\"Label: {label_batch.numpy()[i]} ({raw_train_ds.class_names[label_batch.numpy()[i]]})\"\n",
    "#           f\" => Post: {text_batch.numpy()[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom standardization function to strip HTML break tags '<br />'.\n",
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "    stripped_punct = tf.strings.regex_replace(stripped_html, '[%s]' % re.escape(string.punctuation), '')\n",
    "    return stripped_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary size (features) and number of words in a sequence (review).\n",
    "# max_features = 10000\n",
    "# sequence_length = 250\n",
    "max_features = 15000\n",
    "sequence_length = 350\n",
    "\n",
    "# Use the text vectorization layer to normalize, split, and map strings to\n",
    "# integers. Note that the layer uses the custom standardization defined above.\n",
    "# Set maximum_sequence length as all samples are not of the same length.\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "                    standardize=custom_standardization,\n",
    "                    max_tokens=max_features,\n",
    "                    output_mode='int',\n",
    "                    output_sequence_length=sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
    "train_text = raw_train_ds.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(text, label):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return vectorize_layer(text), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # retrieve a batch (of 32 reviews and labels) from the dataset\n",
    "# text_batch, label_batch = next(iter(raw_train_ds))\n",
    "# first_review, first_label = text_batch[0], label_batch[0]\n",
    "# print(\"Review\", first_review)\n",
    "# print(\"Label\", raw_train_ds.class_names[first_label])\n",
    "# print(\"Vectorized review\", vectorize_text(first_review, first_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"1287 ---> \",vectorize_layer.get_vocabulary()[1287])\n",
    "# print(\" 313 ---> \",vectorize_layer.get_vocabulary()[313])\n",
    "# print('Vocabulary size: {}'.format(len(vectorize_layer.get_vocabulary())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vectorized training, validation, and test datasets\n",
    "train_ds = raw_train_ds.map(vectorize_text)\n",
    "val_ds = raw_val_ds.map(vectorize_text)\n",
    "test_ds = raw_test_ds.map(vectorize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [ x for x in train_ds.take(1).as_numpy_iterator() ]\n",
    "# print(dir(train_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the Dataset for Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a function to build and compile a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "    model = tf.keras.Sequential()\n",
    "        \n",
    "    # Tune the embedding dimensions\n",
    "    hp_embedding_dims = hp.Int('embedding_dimensions', min_value=16, max_value=64, step=16)\n",
    "    model.add( layers.Embedding( max_features+1, hp_embedding_dims ) )\n",
    "\n",
    "    # Tune the dropout rate in the first Dropout layer\n",
    "    hp_dropout_rate_1 = hp.Float('dropout_rate_1', min_value=0.0, max_value=0.3, step=0.1)\n",
    "    model.add( layers.Dropout(hp_dropout_rate_1) )\n",
    "    \n",
    "    model.add( layers.GlobalAveragePooling1D() )\n",
    "\n",
    "    # Tune the dropout rate in the second Dropout layer\n",
    "    hp_dropout_rate_2 = hp.Float('dropout_rate_2', min_value=0.0, max_value=0.3, step=0.1)\n",
    "    model.add( layers.Dropout(hp_dropout_rate_2) )\n",
    "    \n",
    "    model.add( layers.Dense(4) )\n",
    "    \n",
    "    # Tune the learning rate for the optimizer\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam( learning_rate=hp_learning_rate ),\n",
    "        loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['accuracy']\n",
    "        )    \n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_dim = 32\n",
    "# model = tf.keras.Sequential( [\n",
    "#     layers.Embedding( max_features+1, embedding_dim ),\n",
    "#     layers.Dropout(0.2),\n",
    "#     layers.GlobalAveragePooling1D(),\n",
    "#     layers.Dropout(0.2),\n",
    "#     layers.Dense(4)\n",
    "#     ])\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile and Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared Tensorboard log dir: .\\logs\\fit_20211119-153512\n"
     ]
    }
   ],
   "source": [
    "# tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir='logs')\n",
    "LOGS_ROOTDIR = os.path.join('.', 'logs')\n",
    "this_run_log_dir = os.path.join( LOGS_ROOTDIR, \"fit_\" + dt.datetime.now().strftime(\"%Y%m%d-%H%M%S\") )\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=this_run_log_dir, histogram_freq=1)\n",
    "print(f\"Prepared Tensorboard log dir: {this_run_log_dir}\")\n",
    "\n",
    "# Stop epochs if validation loss is increasing\n",
    "earlystopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(\n",
    "#     optimizer='adam',\n",
    "#     loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "#     metrics=['accuracy']\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 100\n",
    "# history = model.fit(\n",
    "#     train_ds,\n",
    "#     validation_data=val_ds,\n",
    "#     epochs=epochs,\n",
    "#     callbacks=[tensorboard_callback, earlystopping_callback]\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the Tuner and Perform Hypertuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(model_builder,\n",
    "                     objective='val_accuracy',\n",
    "                     max_epochs=100,\n",
    "                     factor=3,\n",
    "                     directory='hpt_dir',\n",
    "                     project_name='nlp_multiclass_hpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 53 Complete [00h 00m 09s]\n",
      "val_accuracy: 0.29124999046325684\n",
      "\n",
      "Best val_accuracy So Far: 0.7943750023841858\n",
      "Total elapsed time: 00h 07m 10s\n",
      "\n",
      "Search: Running Trial #54\n",
      "\n",
      "Hyperparameter    |Value             |Best Value So Far \n",
      "embedding_dimen...|32                |48                \n",
      "dropout_rate_1    |0.2               |0.2               \n",
      "dropout_rate_2    |0.2               |0.2               \n",
      "learning_rate     |0.001             |0.01              \n",
      "tuner/epochs      |2                 |2                 \n",
      "tuner/initial_e...|0                 |0                 \n",
      "tuner/bracket     |4                 |4                 \n",
      "tuner/round       |0                 |0                 \n",
      "\n",
      "Epoch 1/2\n"
     ]
    }
   ],
   "source": [
    "tuner.search(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=100,\n",
    "    callbacks=[earlystopping_callback]\n",
    "    )\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters( num_trials=1 )[0]\n",
    "print(f\"Optimal hyperparameters:\\n{best_hps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop epochs if validation loss is increasing\n",
    "earlystopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "# Train the model with the optimal hyperparameters obtained from the tuner search above\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=100,\n",
    "    callbacks=[tensorboard_callback, earlystopping_callback]\n",
    "    )\n",
    "\n",
    "val_acc_per_epoch = history.history['val_accuracy']\n",
    "best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
    "print('Best epoch: %d' % (best_epoch,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinstantiate and retrain the model with the\n",
    "# optimal hyperparameters and optimal number of epochs determined above\n",
    "hypermodel = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=best_epoch,\n",
    "    callbacks=[tensorboard_callback]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = hypermodel.evaluate(test_ds)\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "history_dict = history.history\n",
    "print( history_dict.keys() )\n",
    "\n",
    "plt.plot( 'accuracy', 'b-', data=history.history )\n",
    "plt.plot( 'val_accuracy', 'g:', data=history.history )\n",
    "# plt.plot( [accuracy]*len(history.history['binary_accuracy']), 'r--', data=history.history )\n",
    "y_axis_min_val = max( 0, round(min(accuracy, min(history.history['val_accuracy']), min(history.history['accuracy']) )-0.1, 1) )\n",
    "_ = plt.ylim( y_axis_min_val, 1.0)\n",
    "plt.axhline( accuracy, color='r', linestyle=':')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the trained model to permit it to be used directly in input strings (vs. vectors)\n",
    "# and then apply an activation layer to yield a floating result between 0 and 1\n",
    "export_model = tf.keras.Sequential([\n",
    "    vectorize_layer,\n",
    "    hypermodel,\n",
    "    layers.Softmax()\n",
    "    ])\n",
    "\n",
    "export_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test it with `raw_test_ds`, which yields raw strings\n",
    "loss, accuracy = export_model.evaluate(raw_test_ds)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some test examples\n",
    "n_examples = 16\n",
    "examples_np = [ x for x in raw_test_ds.take(n_examples).as_numpy_iterator() ]\n",
    "examples = [ x[0][0].decode() for x in examples_np ]\n",
    "true_labels = [ x[1][0] for x in examples_np ]\n",
    "true_classes = [ raw_train_ds.class_names[label] for label in true_labels ]\n",
    "\n",
    "preds = export_model.predict(examples)\n",
    "pred_labels = list(np.argmax(preds, axis=1))\n",
    "pred_classes = [ raw_train_ds.class_names[label] for label in pred_labels ]\n",
    "r_df = pd.concat([\n",
    "        pd.DataFrame( {'Post': examples,\n",
    "                       'True Label': true_labels, 'True Class': true_classes,\n",
    "                       'Predicted Label': pred_labels, 'Predicted Class': pred_classes\n",
    "                      }),\n",
    "        pd.DataFrame( preds, columns=['c0 (csharp)', 'c1 (java)', 'c2 (javascript)', 'c3 (python)'] ),\n",
    "        pd.DataFrame( np.max(preds, axis=1) / np.mean(preds, axis=1), columns=['max/mean'] )\n",
    "        ], axis=1 )\n",
    "r_df['Prediction Correct'] = r_df['Predicted Label'] == r_df['True Label']\n",
    "r_df = r_df[['Post', 'True Label', 'True Class', 'Predicted Label',\n",
    "       'Predicted Class', 'c0 (csharp)', 'c1 (java)', 'c2 (javascript)',\n",
    "       'c3 (python)', 'max/mean', 'Prediction Correct']]\n",
    "r_df.sort_values(by=['Prediction Correct', 'max/mean'], ascending=[False, False], inplace=True)\n",
    "r_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve the Trained Word Embeddings and Save them to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights = model.get_layer('embedding').get_weights()[0]\n",
    "# vocab = vectorize_layer.get_vocabulary()\n",
    "\n",
    "# with io.open('./data/vectors.tsv', 'w', encoding='utf-8') as out_v:\n",
    "#     with io.open('./data/metadata.tsv', 'w', encoding='utf-8') as out_m:\n",
    "#         for index, word in enumerate(vocab):\n",
    "#             if index == 0:\n",
    "#                 continue    # skip 0, it's padding\n",
    "                \n",
    "#             vec = weights[index]\n",
    "#             out_v.write( '\\t'.join( [ str(x) for x in vec ]) + '\\n' )\n",
    "#             out_m.write( word + '\\n' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Embeddings\n",
    "* Tensorflow Embedding Projector: http://projector.tensorflow.org/\n",
    "* Click on \"Load data\".\n",
    "* Upload the two files you created above: vecs.tsv and meta.tsv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a32e172698e50824da0c33bd5d955f0bc05a5b3df854aad51c6c7dfa51d03fde"
  },
  "kernelspec": {
   "display_name": "Python [conda env:py38]",
   "language": "python",
   "name": "conda-env-py38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
